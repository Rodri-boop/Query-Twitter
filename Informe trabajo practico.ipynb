{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385c57e8",
   "metadata": {},
   "source": [
    "<h1><center><u>Trabajo Practico Estructuras de datos</u></center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105dfa78",
   "metadata": {},
   "source": [
    "<h2><u>Primera parte:</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e1b1c",
   "metadata": {},
   "source": [
    "<h4><font  size=\"3\" face=\"Times new romans\">•El trabajo practico consiste en la recolección masiva de Tweets sobre un tema de actualidad. Nuestra selección fue todo lo referido a la pandemia, covid-19 y la vacunación. Esto lo empleamos en nuestra QUERY ampliada, sobre el live Stream visto en la clase 7 \"Redes sociales\", el cual modificamos de tal forma que cada vez que encuentra un tweet según la query, tome los datos del usuario, fecha del tweet, su id y el texto almacenándolos en un archivo .json llamado corpus.</font></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b57008b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200] START...\n",
      "La busqueda empezo a la/s --> 21:17:57\n",
      "\n",
      "Done!\n",
      "La busqueda termino a la/s --> 21:18:01\n",
      "Tweets encontrados --> 0\n",
      "Tamanio total de los tweets --> 0 bytes\n",
      "Cantidad total de tweets almacenados: 121\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import time\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    from TwitterAPI import TwitterAPI, TwitterOAuth, TwitterRequestError, TwitterConnectionError, HydrateType, OAuthType\n",
    "    import json\n",
    "\n",
    "    def stream_tweets(query, expansions, tweet_fields, user_fields):\n",
    "        \n",
    "        try:\n",
    "            o = TwitterOAuth.read_file()\n",
    "            api = TwitterAPI(o.consumer_key, o.consumer_secret, auth_type=OAuthType.OAUTH2, api_version='2')\n",
    "\n",
    "            r = api.request('tweets/search/stream/rules', {'add': [{'value':QUERY}]})\n",
    "            json.dumps(r.json(), indent=2)\n",
    "            if r.status_code != 201: exit()\n",
    "\n",
    "            r = api.request('tweets/search/stream/rules', method_override='GET')\n",
    "            json.dumps(r.json(), indent=2)\n",
    "            if r.status_code != 200: exit()\n",
    "\n",
    "            r = api.request('tweets/search/stream', {\n",
    "                    'expansions': expansions,\n",
    "                    'tweet.fields': tweet_fields,\n",
    "                    'user.fields': user_fields,\n",
    "                },\n",
    "                hydrate_type=HydrateType.APPEND)\n",
    "\n",
    "            print(f'[{r.status_code}] START...')\n",
    "            print(\"La busqueda empezo a la/s --> \" + time.strftime('%H:%M:%S', time.localtime()))\n",
    "            \n",
    "            contador_de_tweets = 0\n",
    "            contador_de_tamanio = 0\n",
    "            info = {}\n",
    "            for item in r:\n",
    "                for c,v in item.items():\n",
    "                    try:\n",
    "                        info[v['conversation_id']] = {'texto' : v['text'], 'nombre_oficial' : v['author_id_hydrate']['name'], 'cuenta' : v['author_id_hydrate']['username'], 'id_usuario' : v['author_id'], 'fecha' : v['created_at']}\n",
    "                        if 'location' in v['author_id_hydrate']:\n",
    "                            info[v['conversation_id']]['ubicacion'] = v['author_id_hydrate']['location']\n",
    "                        if 'source' in v:\n",
    "                            info[v['conversation_id']]['fuente'] = v['source']\n",
    "                        contador_de_tweets += 1\n",
    "                        print(\"Tweets encontrados hasta el momento --> \" + str(contador_de_tweets))\n",
    "                        contador_de_tamanio += sys.getsizeof(info[v['conversation_id']])\n",
    "                        print(\"Tamano del tweet hasta el momento -->  \" + str(contador_de_tamanio) + ' bytes\\n')\n",
    "\n",
    "                        if contador_de_tweets == 1000:\n",
    "                            contador_de_tweets = 0\n",
    "                            with open('corpus.json',\"r\", encoding=\"utf-8\") as file_corpus:\n",
    "                                datos_previos = json.load(file_corpus)\n",
    "                                datos_previos.update(info)\n",
    "                                print('-------------------------------------------')\n",
    "                                print(\"BACKUP: Cantidad total de tweets almacenados: \"+str(len(datos_previos)))\n",
    "                                print('-------------------------------------------'+ '\\n')\n",
    "\n",
    "                            with open('corpus.json',\"w\", encoding=\"utf-8\") as file_corpus:\n",
    "                                jsonobj = json.dumps(datos_previos, indent=2)\n",
    "                                file_corpus.write(jsonobj)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            print('\\nDone!')\n",
    "            print(\"La busqueda termino a la/s --> \" + time.strftime('%H:%M:%S', time.localtime()))\n",
    "            print(\"Tweets encontrados --> \" + str(contador_de_tweets))\n",
    "            print(\"Tamanio total de los tweets --> \" + str(contador_de_tamanio)+ ' bytes')\n",
    "            try:\n",
    "                with open('corpus.json',\"r\", encoding=\"utf-8\") as file_corpus:\n",
    "                    datos_previos = json.load(file_corpus)\n",
    "                    datos_previos.update(info)\n",
    "                    print(\"Cantidad total de tweets almacenados: \"+str(len(datos_previos)))\n",
    "\n",
    "                with open('corpus.json',\"w\", encoding=\"utf-8\") as file_corpus:\n",
    "                    jsonobj = json.dumps(datos_previos, indent=2)\n",
    "                    file_corpus.write(jsonobj)\n",
    "            except FileNotFoundError as e:\n",
    "                with open('corpus.json',\"w\", encoding=\"utf-8\") as file_corpus:\n",
    "                    jsonobj = json.dumps(info, indent=2)\n",
    "                    file_corpus.write(jsonobj)\n",
    "\n",
    "        except TwitterRequestError as e:\n",
    "            print(f'\\n{e.status_code}')\n",
    "            for msg in iter(e):\n",
    "                print(msg)\n",
    "\n",
    "        except TwitterConnectionError as e:\n",
    "            print(e)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    QUERY = '(vacuna OR covid OR pandemia OR #covid-19 OR #covid19 OR Sputnik OR Astrazeneca OR Sinopharm) (-muerte) lang:es (-is:retweet) (-is:reply)'\n",
    "    EXPANSIONS = 'author_id,referenced_tweets.id,referenced_tweets.id.author_id,in_reply_to_user_id,attachments.media_keys,attachments.poll_ids,geo.place_id,entities.mentions.username'\n",
    "    TWEET_FIELDS='author_id,conversation_id,created_at,entities,geo,id,lang,public_metrics,source,text'\n",
    "    USER_FIELDS='created_at,description,entities,location,name,profile_image_url,public_metrics,url,username'\n",
    "    stream_tweets(QUERY, EXPANSIONS, TWEET_FIELDS, USER_FIELDS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694a9b38",
   "metadata": {},
   "source": [
    "<h4><font  size=\"3\" face=\"Times new romans\">•Tomando el código visto en clase sobre BSBI y optimizarlo para reducir sus coste de ejecución, pudimos construir nuestro indice invertido. A partir de nuestros corpus.json realizamos nuestros diccionarios de documentos(ubicación del corpus que contiene toda la información de cada tweet) y nuestros postings.json(que guardan las id de cada termino apuntando los documentos donde aparecen y la id de cada tweet).</font></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53a15e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "class Indice_BSBI(object):\n",
    "    def __init__(self, salida, temporal, documentos, blocksize):\n",
    "        self.documentos = documentos\n",
    "        self.salida = salida\n",
    "        self._blocksize = blocksize\n",
    "        self._temp = temporal\n",
    "        \n",
    "    def generar_docID(self):\n",
    "            doc_a_docID = {}\n",
    "            docID_to_doc = {}\n",
    "            docs_list = re.findall(r'(?<=Practico\\/)\\S*', self.documentos)\n",
    "\n",
    "            for i in range(len(docs_list)):\n",
    "                doc_a_docID[docs_list[i]] = i\n",
    "            self.docs_list = docs_list\n",
    "            self._doc_a_docID = doc_a_docID\n",
    "\n",
    "    def invertir_bloque(self, bloque):\n",
    "        bloque_invertido = {}\n",
    "        bloque_ordenado = sorted(bloque, key=lambda tuple: (tuple[0], tuple[1]))\n",
    "        for par in bloque_ordenado:\n",
    "            posting = bloque_invertido.setdefault(par[0], set())\n",
    "            posting.add(par[1])\n",
    "        return bloque_invertido\n",
    "\n",
    "    def guardar_bloque_invertido(self, bloque, numero_bloque):\n",
    "        archivo_de_salida = \"b\"+str(numero_bloque)+\".json\"\n",
    "        archivo_de_salida = os.path.join(self._temp, archivo_de_salida)\n",
    "        for clave in bloque:\n",
    "            bloque[clave] = list(bloque[clave])\n",
    "        with open(archivo_de_salida, \"w\") as contenedor:\n",
    "            json.dump(bloque, contenedor)\n",
    "        return archivo_de_salida\n",
    "\n",
    "    def guardar_dict_docs(self):\n",
    "        self._doc_a_docID = { doc_id : doc for doc, doc_id in self._doc_a_docID.items()}\n",
    "        path = os.path.join(self.salida, \"dict_docs.json\")\n",
    "        with open(path, \"w\") as contenedor:\n",
    "            json.dump(self._doc_a_docID, contenedor)\n",
    "    \n",
    "    def guardar_index_dict(self, dict_path, dict):\n",
    "        path = os.path.join(self.salida, dict_path)\n",
    "        with open(path, \"w\") as container:\n",
    "            json.dump(dict, container)\n",
    "\n",
    "    def intercalar_bloques(self, archivos_temp, archivos_ID):\n",
    "        lista_term_ID=[str(i) for i in range(len(archivos_ID))]\n",
    "        posting = os.path.join(self.salida,\"postings.json\")\n",
    "        lista_de_bloques=[]\n",
    "        dict_postings = {}\n",
    "\n",
    "        open_files = [open(f, \"r\") for f in archivos_temp]\n",
    "        for data in open_files:\n",
    "            data.seek(0)\n",
    "            lista_de_bloques.append(json.load(data))\n",
    "        \n",
    "        with open(posting,\"w\") as output:\n",
    "            for termID in lista_term_ID:\n",
    "                dict_docID_tweetID = {}\n",
    "                for bloque in lista_de_bloques:\n",
    "                    try:\n",
    "                        for lista in bloque[termID]:\n",
    "                            if lista[0] not in dict_docID_tweetID.keys():\n",
    "                                dict_docID_tweetID[lista[0]] = []\n",
    "                            dict_docID_tweetID[lista[0]].append(lista[1])\n",
    "                    except:\n",
    "                        pass\n",
    "                dict_postings.update({termID : dict_docID_tweetID})\n",
    "            output.write(json.dumps(dict_postings, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae0ac05",
   "metadata": {},
   "source": [
    "<h4><font  size=\"3\" face=\"Times new romans\">•Importamos nuestro BSBI para crear dos clases hijas, una que nos construye un diccionario de terminos:id, un diccionario de documentos:id y un diccionario 'postings' que tiene como clave la id de los terminos y como valor un diccionario con id_doc:id_tweet.</font></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64667497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from BSBI import Indice_BSBI\n",
    "\n",
    "class Indice_de_palabras(Indice_BSBI):\n",
    "    def __init__(self, salida, temporal, documentos, blocksize, language='spanish'):\n",
    "        super().__init__(salida, temporal, documentos,blocksize)\n",
    "        self._stop_words = frozenset(stopwords.words(language))\n",
    "        self._stemmer = SnowballStemmer(language, ignore_stopwords=False)\n",
    "        self._term_to_termID = {}\n",
    "\n",
    "        self.generar_docID()\n",
    "        self.__indexar()\n",
    "\n",
    "\n",
    "    def __lemmatizar(self, palabra):\n",
    "        palabra = palabra.strip(string.punctuation + \"|\" + \"'\" + \"´\" + \"-\" + \"»\" + \"\\x97\" + \"¿\" + \"¡\" +\\\n",
    "                                \"\\u201c\" + \"\\u25b6\" + \"\\u201d\" + \"\\u2014\" + \"\\u2018\" + \"\\u2019\" + \"\\u00bf\")\n",
    "\n",
    "        lemmatized_word = self._stemmer.stem(palabra)\n",
    "        return lemmatized_word\n",
    "\n",
    "    def __indexar(self):\n",
    "        n = 0\n",
    "        lista_de_bloques = []\n",
    "\n",
    "        for bloque in self.__parse_next_block():\n",
    "            bloque_invertido = self.invertir_bloque(bloque)\n",
    "            lista_de_bloques.append(self.guardar_bloque_invertido(bloque_invertido, n))\n",
    "            n += 1\n",
    "        start = time.process_time()\n",
    "        self.intercalar_bloques(lista_de_bloques, self._term_to_termID)\n",
    "        end = time.process_time()\n",
    "        print(\"Intercalar bloques de palabras ==> Tiempo transcurrido: \", end-start)\n",
    "\n",
    "        self.guardar_dict_terms()\n",
    "        self.guardar_dict_docs()\n",
    "\n",
    "    def __parse_next_block(self):\n",
    "        blocksize = self._blocksize\n",
    "        termID = 0\n",
    "        bloque = []\n",
    "        for doc in self.docs_list:\n",
    "            try:\n",
    "                with open(doc, \"r\", encoding=\"utf-8\") as corpus:\n",
    "                    dict_tweet = json.load(corpus)\n",
    "                    for id, data in dict_tweet.items():\n",
    "\n",
    "                        blocksize -= len(data['texto'].encode('utf-8'))\n",
    "                        tweet = self.__sacar_links_y_emojis(data['texto'])\n",
    "                        for palabra in tweet:\n",
    "                            \n",
    "                            if palabra not in self._stop_words and len(palabra) > 1:\n",
    "                                palabra = self.__lemmatizar(palabra)\n",
    "                                if palabra not in self._term_to_termID:\n",
    "                                    self._term_to_termID[palabra] = termID\n",
    "                                    termID += 1\n",
    "                                bloque.append((self._term_to_termID[palabra],(self._doc_a_docID[doc], id )))\n",
    "\n",
    "                        if blocksize <= 0:\n",
    "                            yield bloque\n",
    "                            blocksize = self._blocksize\n",
    "                            bloque = []\n",
    "                    yield bloque\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    def __sacar_links_y_emojis(self, tweet):\n",
    "        regex_link = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "        regex_emoji = '[\\u2620-\\U0001fa7a]|[\\U000e0067-\\U000e007f]|[\\U000e0062\\U000e0065]\\\n",
    "                        |[\\U0001fa7a-\\U0001fac0]+|[\\u200d]+|[\\U0001f4bc]+'\n",
    "        regex_mention = '@[a-zA-Z0-9-]*|#[a-zA-Z0-9À-ÿ\\u00f1\\u00d1$-_@.&+]*'\n",
    "\n",
    "        links = set(re.findall(regex_link, tweet))\n",
    "        emojis = set(re.findall(regex_emoji, tweet))\n",
    "        menciones = set(re.findall(regex_mention, tweet))\n",
    "        set_de_caracteres = links.union(emojis.union(menciones))\n",
    "\n",
    "        for caracter in set_de_caracteres:\n",
    "            tweet = tweet.replace(caracter, ' ')\n",
    "\n",
    "        return(tweet.split())\n",
    "\n",
    "    def guardar_dict_terms(self):\n",
    "        super().guardar_index_dict(\"dict_terms.json\", self._term_to_termID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b7f239",
   "metadata": {},
   "source": [
    "<h4><font  size=\"3\" face=\"Times new romans\">•La otra clase hija construye un diccionario de fecha:id, un diccionario de documentos:id y un diccionario 'postings' que tiene como clave el id de las fechas y como valor un diccionario con id_doc: id_tweet.</font></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from BSBI import Indice_BSBI\n",
    "\n",
    "class Indice_de_fechas(Indice_BSBI):\n",
    "    def __init__(self, salida, temporal, documentos, blocksize):\n",
    "        super().__init__(salida, temporal, documentos, blocksize)\n",
    "        self._fecha_a_fechaID = {}\n",
    "\n",
    "        self.generar_docID()\n",
    "        self.__indexar()\n",
    "\n",
    "    def __indexar(self):\n",
    "        n = 0\n",
    "        lista_de_bloques = []\n",
    "\n",
    "        for bloque in self.__parse_next_block():\n",
    "            bloque_invertido = self.invertir_bloque(bloque)\n",
    "            lista_de_bloques.append(self.guardar_bloque_invertido(bloque_invertido, n))\n",
    "            n += 1\n",
    "        start = time.process_time()\n",
    "        self.intercalar_bloques(lista_de_bloques, self._fecha_a_fechaID)\n",
    "        end = time.process_time()\n",
    "        print(\"Intercalar bloques de fechas ==> Tiempo transcurrido: \", end-start)\n",
    "\n",
    "        self.guardar_dict_fechas()\n",
    "        self.guardar_dict_docs()\n",
    "    \n",
    "    def guardar_dict_fechas(self):\n",
    "        super().guardar_index_dict(\"dict_dates.json\", self._fecha_a_fechaID)\n",
    "\n",
    "    def __parse_next_block(self):\n",
    "        blocksize = self._blocksize\n",
    "        dateID = 0\n",
    "        bloque = []\n",
    "        for doc in self.docs_list:\n",
    "            try:\n",
    "                with open(doc, \"r\", encoding=\"utf-8\") as file_corpus:\n",
    "                    dict_tweet = json.load(file_corpus)\n",
    "                    for id, data in dict_tweet.items():\n",
    "                        blocksize -= len(data['texto'].encode('utf-8'))\n",
    "                        date = self.__limpiar_fechas(data['fecha'])\n",
    "                        if date not in self._fecha_a_fechaID:\n",
    "                            self._fecha_a_fechaID[date] = dateID\n",
    "                            dateID += 1\n",
    "                        bloque.append((self._fecha_a_fechaID[date],(self._doc_a_docID[doc], id)))\n",
    "\n",
    "                        if blocksize <= 0:\n",
    "                            yield bloque\n",
    "                            blocksize = self._blocksize\n",
    "                            bloque = []\n",
    "                    yield bloque\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    def __limpiar_fechas(self, tweet):\n",
    "        fecha = tweet[:4] +'-'+ tweet[5:7] +'-'+ tweet[8:10] +' '+ tweet[11:13] +':'+ tweet[14:16]\n",
    "        return fecha\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eca30e",
   "metadata": {},
   "source": [
    "<h2><u>Segunda parte:</u></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d5a7ad",
   "metadata": {},
   "source": [
    "<font  size=\"3\" face=\"Times new romans\">•Para el menu de busqueda de tweets decidimos utilizar la interfaz de tkinter, creando diferentes ventanas la principal que nos brinda las opciones de ingresar los documentas con los cuales formamos cada uno de los indices.</font>\n",
    "\n",
    "![titulo](VentanaPrincipal.png )\n",
    "\n",
    "\n",
    "\n",
    "<font  size=\"3\" face=\"Times new romans\">•La ventana de búsqueda por palabras, cuenta con la opción de ingresar diferentes operadores tanto intersección de palabras, diferencia de palabras y conjunción de palabras. Para realizar esto le matiza las palabras ingresadas, con las cuales implementa una lista de diccionarios con la clave del id del documento y de valor un conjunto con todos los id_tweets del termino en ese documento.</font>\n",
    "\n",
    "\n",
    "![titulo](VentanaBusquedaPalabra.png )\n",
    "\n",
    "\n",
    "<font  size=\"3\" face=\"Times new romans\">•La ventana de busqueda por fechas, permite la busqueda entre dos fechas que pueden ser ingresadas en el formato AAA-MM-DD. Las fechas ingresan en un formato string, luego de ser spliteadas se las une en un formato integer para poder compararlas entre si. Luego se recorre el diccionario de fechas:id con las fechas desde y hasta obtiene todos los valores entre ese rango, con estas id se dirije al posting para obtener el todos los id_tweets y de los documentos donde se encuentra para finalizar, en el corpus devolviendo los tweets entre el rango pedido.</font>\n",
    "\n",
    "![titulo](VentanaBusquedaFecha.png )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47297390",
   "metadata": {},
   "source": [
    "<font  size=\"3\" face=\"Times new romans\">•El menu de operaciones es con el cual nosotros podemos realizar la busqueda de las palabras ingresadas y devolver una cantidad pedida de los tweets con estas palabras.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import string\n",
    "import tkinter as tk\n",
    "from tkinter import Listbox, Scrollbar, Tk, ttk\n",
    "from tkinter import filedialog as fd\n",
    "from tkinter.constants import BOTH, END, LEFT, RADIOBUTTON, RIGHT, X, Y\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class Menu_de_operaciones:\n",
    "    def __init__(self):\n",
    "        self._stop_words = frozenset(stopwords.words('spanish'))\n",
    "        self._stemmer = SnowballStemmer('spanish', ignore_stopwords=False)\n",
    "\n",
    "    def aceptar_fechas(self,d,h,cantidad):\n",
    "        with open (\"output_dates/dict_dates.json\",\"r\",encoding=\"UTF-8\") as dict_dates,\\\n",
    "            open (\"output_dates/postings.json\",\"r\",encoding=\"UTF-8\") as posting,\\\n",
    "            open (\"output_dates/dict_docs.json\",\"r\",encoding=\"UTF-8\") as dict_docs:\n",
    "            self.dict_da= json.load(dict_dates)\n",
    "            self.post= json.load(posting)\n",
    "            self.dict_doc=json.load(dict_docs)\n",
    "\n",
    "        desde= self.pasaje_fecha_a_numero(d)\n",
    "        hasta= self.pasaje_fecha_a_numero(h)\n",
    "\n",
    "        self.lista_completa = []\n",
    "        for c,v in self.dict_da.items():\n",
    "            clave= self.pasaje_fecha_a_numero(c)\n",
    "            if clave >=desde and clave<=hasta:\n",
    "                self.obtener_id(str(v))\n",
    "        self.ventana_nueva_fechas(self.lista_completa,cantidad)\n",
    "\n",
    "    def obtener_id(self,valor):\n",
    "        dict_apariciones=self.post[valor]\n",
    "\n",
    "        for doc_id,list_tweets in dict_apariciones.items():\n",
    "            with open (str(self.dict_doc[str(doc_id)]),\"r\",encoding=\"UTF-8\") as corpus:\n",
    "                self.corpus=json.load(corpus)\n",
    "                for id in list_tweets:\n",
    "                    self.lista_completa.append(id)\n",
    "            \n",
    "    def pasaje_fecha_a_numero(self,palabras):\n",
    "        suma=\"\"\n",
    "        lista = re.split(r'[\\W]', palabras)\n",
    "        for num in lista:\n",
    "            suma+=num\n",
    "        return int(suma)\n",
    "    \n",
    "\n",
    "    def ventana_nueva_fechas(self, list_tweets, cantidad_de_tweets_pedidos):\n",
    "            self.ventana1 = Tk()\n",
    "            self.ventana1.geometry('600x680')\n",
    "            self.ventana1.resizable(False,False)\n",
    "            self.ventana1.iconbitmap(\"Twitter.ico\")\n",
    "            self.labelframe1=ttk.LabelFrame(self.ventana1, text=\"Búsqueda\")        \n",
    "            self.labelframe1.grid(column=0, row=0, padx=5, pady=10)\n",
    "            scrollbar = Scrollbar(self.labelframe1)\n",
    "            scrollbar.pack( side = RIGHT, fill = Y )\n",
    "\n",
    "            mylist = Listbox(self.labelframe1, yscrollcommand = scrollbar.set, width=95, height=40)\n",
    "            cantidad_de_tweets = int(cantidad_de_tweets_pedidos)\n",
    "            if(len(list_tweets)<cantidad_de_tweets-1):\n",
    "                cantidad_de_tweets = len(list_tweets)-1\n",
    "            \n",
    "            for id_tweet in list_tweets[0:int(cantidad_de_tweets_pedidos)]:\n",
    "                tweet = (\"@{} => {}\".format(self.corpus[id_tweet][\"cuenta\"], self.corpus[id_tweet][\"texto\"]))\n",
    "                largo = len(tweet.encode('utf-8'))\n",
    "                mylist.insert(END,'---------------------------------------------------------------------------------------------------------------')\n",
    "                aux = 0\n",
    "                for x in range(0,largo//100):\n",
    "                    mylist.insert(END,tweet[aux:aux+100])\n",
    "                    aux=aux+100\n",
    "                mylist.insert(END,tweet[aux:])\n",
    "            mylist.insert(END,'---------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "            mylist.pack( side = LEFT, fill = BOTH )\n",
    "            scrollbar.config( command = mylist.yview )\n",
    "\n",
    "            self.ventana1.mainloop()\n",
    "    \n",
    "    def aceptar_palabras(self,entrada,negadas,cantidad):\n",
    "        with open (\"output_words/dict_terms.json\",\"r\",encoding=\"UTF-8\") as dict_terms,\\\n",
    "            open (\"output_words/postings.json\",\"r\",encoding=\"UTF-8\") as posting,\\\n",
    "            open (\"output_words/dict_docs.json\",\"r\",encoding=\"UTF-8\") as dict_docs:\n",
    "            self.dict_term= json.load(dict_terms)\n",
    "            self.post= json.load(posting)\n",
    "            self.dict_doc=json.load(dict_docs)\n",
    "\n",
    "        lista_dicts_conjuntos = self.conjuntos_id_tweets(entrada)\n",
    "    \n",
    "        operacion_final= self.crear_operacion(entrada,negadas)\n",
    "\n",
    "        self.lista_completa_de_tuplas = []\n",
    "        contador= 0\n",
    "        for tweet in operacion_final:\n",
    "            encontrado = False\n",
    "            for dict in lista_dicts_conjuntos:\n",
    "                for c,v in dict.items():\n",
    "                    if(encontrado == False):\n",
    "                        for id in v:\n",
    "                            if (int(id) == tweet) & (contador< int(cantidad)):\n",
    "                                contador+=1\n",
    "                                with open(self.dict_doc[c],\"r\",encoding=\"UTF-8\") as corpus:\n",
    "                                    self.corpus = json.load(corpus)\n",
    "                                    self.lista_completa_de_tuplas.append((self.corpus[id][\"cuenta\"], self.corpus[id][\"texto\"]))\n",
    "                                encontrado = True\n",
    "            \n",
    "        self.ventana_nueva_palabras(self.lista_completa_de_tuplas)\n",
    "        \n",
    "    def ventana_nueva_palabras(self, lista_de_tuplas):\n",
    "        \n",
    "        self.ventana2 = Tk()\n",
    "        self.ventana2.geometry('600x680')\n",
    "        self.ventana2.resizable(False,False)\n",
    "        self.ventana2.iconbitmap(\"Twitter.ico\")\n",
    "        self.labelframe2=ttk.LabelFrame(self.ventana2, text=\"Búsqueda\")        \n",
    "        self.labelframe2.grid(column=0, row=0, padx=5, pady=10)\n",
    "        scrollbar = Scrollbar(self.labelframe2)\n",
    "        scrollbar.pack( side = RIGHT, fill = Y )\n",
    "\n",
    "        mylist2 = Listbox(self.labelframe2, yscrollcommand = scrollbar.set, width=95, height=40)\n",
    "\n",
    "        for par in lista_de_tuplas:\n",
    "            linea = (\"@{} => {}\".format(par[0], par[1]))\n",
    "            largo = len(linea.encode('utf-8'))\n",
    "            mylist2.insert(END,'---------------------------------------------------------------------------------------------------------------')\n",
    "            aux = 0\n",
    "            for x in range(0,largo//100):\n",
    "                mylist2.insert(END,linea[aux:aux+100])\n",
    "                aux=aux+100\n",
    "            mylist2.insert(END,linea[aux:])\n",
    "        mylist2.insert(END,'-------------------------------------------------------------------------------------------------------------------')    \n",
    "        \n",
    "        mylist2.pack( side = LEFT, fill = BOTH )\n",
    "        scrollbar.config( command = mylist2.yview )\n",
    "\n",
    "        self.ventana2.mainloop()\n",
    "\n",
    "    def conjuntos_id_tweets(self,entrada):\n",
    "        lista_lematizadas = [self.lematizar(palabra) for palabra in re.split(\"[A-Z\\s]+\", entrada)]\n",
    "    \n",
    "        lista_dicts = []\n",
    "        for palabra in lista_lematizadas:\n",
    "            dict_doc_sets = {}\n",
    "            id = self.dict_term[palabra]\n",
    "            dict_doc_tweet = self.post[str(id)]\n",
    "            for c,v in dict_doc_tweet.items():\n",
    "                conjunto = set()\n",
    "                for id_tweet in v:\n",
    "                    conjunto.add(id_tweet)\n",
    "                    dict_doc_sets[c] = conjunto\n",
    "            lista_dicts.append(dict_doc_sets)\n",
    "        return lista_dicts\n",
    "\n",
    "    def conjunto_id_negadas(self,negadas):\n",
    "        lista_negadas_lematizadas = [self.lematizar(palabra) for palabra in negadas.split(' ')]\n",
    "\n",
    "        conjunto_id_negadas = set()\n",
    "        for palabra in lista_negadas_lematizadas:\n",
    "            id = self.dict_term[palabra]\n",
    "            for c,v in self.post[str(id)].items():\n",
    "                for id_tweets in v:\n",
    "                    conjunto_id_negadas.add(int(id_tweets))\n",
    "        return conjunto_id_negadas\n",
    "        \n",
    "    def crear_operacion(self,entrada,negadas):\n",
    "        simbolos = ['(',')','A','N','D','O','R',' ']\n",
    "        operacion_final = ''\n",
    "        numero_de_palabra = 0\n",
    "        posicion=0\n",
    "        entrada+=\" \"\n",
    "        lista_dicts_conjuntos = self.conjuntos_id_tweets(entrada)\n",
    "\n",
    "        while posicion < len(entrada):\n",
    "            if entrada[posicion] not in simbolos:\n",
    "                operacion_final+=\"{\"\n",
    "                for c,v in lista_dicts_conjuntos[numero_de_palabra].items():\n",
    "                    for id_tweet in v:\n",
    "                        operacion_final+= id_tweet\n",
    "                        operacion_final+=\",\"\n",
    "                operacion_final= operacion_final[:-1]\n",
    "                operacion_final+=\"}\"\n",
    "                numero_de_palabra+=1\n",
    "                booleano_posicion=True\n",
    "                while booleano_posicion:\n",
    "                    posicion+=1\n",
    "                    if(entrada[posicion]) in simbolos:\n",
    "                        booleano_posicion=False\n",
    "            else:\n",
    "                operacion_final+=entrada[posicion]\n",
    "                posicion+=1\n",
    "\n",
    "        operacion_final=operacion_final.replace(\"AND\", \"&\")\n",
    "        operacion_final=operacion_final.replace(\"OR\", \"|\")\n",
    "        operacion_final=eval(operacion_final)\n",
    "        operacion_final=operacion_final-self.conjunto_id_negadas(negadas)\n",
    "        return operacion_final    \n",
    "\n",
    "    def lematizar(self, palabra):\n",
    "        palabra = palabra.strip(string.punctuation + \"|\" + \"'\" + \"´\" + \"-\" + \"»\" + \"\\x97\" + \"¿\" + \"¡\" +\\\n",
    "                                \"\\u201c\" + \"\\u25b6\" + \"\\u201d\" + \"\\u2014\" + \"\\u2018\" + \"\\u2019\" + \"\\u00bf\")\n",
    "\n",
    "        palabra_lematizada = self._stemmer.stem(palabra)\n",
    "        return palabra_lematizada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1348dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
